---
title: Evaluation
author: Dr Charles Martin
title-slide-attributes:
    data-background-image: img/aisee-holo-view.jpg
    data-background-size: cover
---
 
## Announcements

<!-- 
Yichen todos:
- look at chapters 14-16, trace the sections through the slides and add slides for missing sections
- write in short summaries of missing sections
- edit other slides as needed for consistency
- include citations.

 -->

## Plan for the class

# Evaluation

Sharp et al. 2019 Textbook: Chapters 14-16

## What is evaluation?

:::::::::::::: {.columns}
::: {.column width="60%"}
- **Evaluation:** collecting and analysing data from user experiences with an artefact.

- **Goal:** to improve the artefact's design.
- **Addresses:**
    - functionality
    - usability
    - user experience
- Appropriate for all different kinds of artefacts and prototypes.
- Methods vary according to goals.
:::
::: {.column width="40%"}
![Evaluating iPad apps in 2013.](img/evaluation-metatone-2013.jpg)
:::
::::::::::::::

## Why is evaluation important?

:::::::::::::: {.columns}
::: {.column width="50%"}
- **Understanding people**
    - Users may not have the same experiences or perspectives as you do
    - Different users use software differently
- **Understanding designs**
    - Proof that ideas work
    - Understand limitations, affordances, applications
:::
::: {.column width="50%"}
- **Business**
    - Invest in the right ideas
    - Find problems to solve (before production, before next iteration, etc.)
- **Research**
    - Evidence for new interactive systems
    - Empirical proof of hypotheses
    - New knowledge to answer research questions
:::
::::::::::::::

## What should you evaluate/measure?

:::::::::::::: {.columns}
::: {.column width="60%"}
> Does the design do what the users need and want?

Examples:

- **Game App Developers:** Whether young adults find their game fun and engaging compared to other games
- **Government authority:** Whether their online service is accessible to users with a disability
- **Children’s talking toy designers:** Whether six-year-olds enjoy the voice, feel of the soft toy, and can use safely
:::
::: {.column width="40%"}
![Preece in @Raffaele:2016](img/09_evaluation_foundations_1.png)
:::
::::::::::::::

## Usability and Usability Goals

:::::::::::::: {.columns}
::: {.column width="60%"}
Six usability goals:

- Effective to use (effectiveness)
- Efficient to use (efficiency)
- Safe to use (safety)
- Having good utility (utility)
- Easy to learn (learnability)
- Easy to remember how to use (memorability)
:::
::: {.column width="40%"}
![Image: dtravisphd on Unsplash](img/09_evaluation_foundations_2.jpg){width=100%}
:::
::::::::::::::

## Where should you evaluate your design?

:::::::::::::: {.columns}
::: {.column width="60%"}
Depends on your evaluation goal!

- Lab studies (controlled settings)
- In-the-wild studies (natural settings)
- Remote studies (online behaviour)

:::
::: {.column width="40%"}
![Image: Unsplash, UX Indonesia](img/09_evaluation_foundations_3.jpg)
:::
::::::::::::::

## Activity: Evaluating an interactive toy

:::::::::::::: {.columns}
::: {.column width="60%"}
You're all HCI researchers and we need to evaluate this interactive toy.

We need to choose:

- how we will evaluate the toy?
- in what environment?
- what information do we need and why?
- what _research questions_ are being asked?

Talk for 2-3 minutes and then we will hear some answers.
:::
::: {.column width="40%"}
![Where and why will we evaluate this toy? (Photo by COSMOH LOVE on Unsplash)](img/cosmoh-love-unlm6Fxxvjw-unsplash.jpg)
:::
::::::::::::::

## When should you evaluate?

:::::::::::::: {.columns}
::: {.column width="60%"}
Evaluation serves different purposes at different stages of the design process

- **Formative evaluation:** 
    - Assessing whether a product continues to meet users’ needs during a design process 
    - Early or late stages
- **Summative evaluation:** 
    - Assessing whether a finished product is successful
    - Feeds into an iterative design process
:::
::: {.column width="40%"}
![Formative vs Summative Evaluation [https://www.youtube.com/watch?v=730UiP7dZeo](https://www.youtube.com/watch?v=730UiP7dZeo)](img/09_evaluation_foundations_4.png)
:::
::::::::::::::

# Types of Evaluation

## Controlled settings (e.g., Usability testing)

![Image Source: [Usability Testing (interactiondesign.org)](https://www.interaction-design.org/literature/article/the-basics-of-recruiting-users-for-usability-testing)](img/09_evaluation_foundations_5.jpg)

## Usability Testing

- Measures: Can involve numbers and time (e.g., number of task completion, number of errors made, time taken to complete task)
- Methods: Can involve a mixture of methods e.g., think aloud, observation, interviews, questionnaires, data logging and analytics
- Data: Can collect a variety of data depending on the methods used (e.g., video, audio, facial expressions, key presses, verbal feedback)
- Settings: Usability lab + observation room vs mobile usability kit
- Number of participants: 5-12 baseline but more is better
- Read the textbook for other kinds of experimental design

## Usability Testing Example

![@schaadhardt-blind-artboards:2021 **Understanding Blind Screen-Reader Users’ Experiences of Digital Artboards**](img/09_evaluation_foundations_6.png)

## Natural settings (e.g., Field studies)

:::::::::::::: {.columns}
::: {.column width="60%"}
Goals of field studies:

- Help identify opportunities for new technology
- Establish the requirements for a new design
- Facilitate the introduction of technology or inform deployment of existing technology in new contexts
:::
::: {.column width="40%"}
![Source: @ambe-individuation:2017](img/09_evaluation_foundations_7.jpg)
:::
::::::::::::::

## Field Studies

- Goals:
    - Understanding how people interact with technologies in “messy worlds”, how technologies will be integrated into contexts
    - Studying use of existing technologies and impacts of introducing new ones
- Methods: Emphasis on qualitative methods rather than statistical measures e.g., Observations, interviews, diaries, interaction logging
- Duration: No fixed length- can be seconds, months, years
- Paying attention to: Use situations, problems/errors, distractions, patterns of behaviours
- How does your presence and involvement shape engagement? Observation vs participant observation
- Findings: Used for creating thematic analysis, vignettes, narratives, critical incident analysis etc.


## Field Studies Example

:::::::::::::: {.columns}
::: {.column width="50%"}
![**Co-Designing with Orangutans: Enhancing the Design of Enrichment for Animals** (Sarah Webber, Marcus Carter, Wally Smith, and Frank Vetere) Proc. DIS '20 [@webber-oranguatans:2020]](img/09_evaluation_foundations_8.png)
:::
::: {.column width="50%"}
![Design objective 1: Develop a digital installation to provide enhanced, varied enrichment for orangutans at Melbourne Zoo](img/09_evaluation_foundations_9.png)
:::
::::::::::::::

## Opportunistic Evaluations

:::::::::::::: {.columns}
::: {.column width="60%"}

Provide quick feedback about a design idea in the early design process.

Confirm whether it’s worth developing an idea into a prototype. It’s informal and doesn’t require many resources.

E.g., designers recruit a few local people for feedback that benefits evolving design.

> Yichen Wang's *arMIDI* system early design / dev process with her supervisor Charles, musician friend Henry Gardner [@wang2025seeingsound].

This can hone target users for subsequent more focused studies, and in addition to the formal.

:::
::: {.column width="10%"}
![](img/armidi-dev-process.jpg)
:::
::::::::::::::

## Which methods to choose from?

:::::::::::::: {.columns}
::: {.column width="60%"}

What we discussed so far are general methods guiding certain dimensions fo developed artefacts.

- Combinations of methods are used for a richer understanding. E.g., usability testing is combined with observations to identify usability problems and how users use the system.

> Pros and Cons

- Controlled settings allow rigorous hypotheses test on the system's specific features for generalised results.
- Uncontrolled settings offer unexpected insights into how people perceive and experience new technologies in their daily and work lives.

:::
::: {.column width="40%"}
![Yichen Wang conducting her AR co-creative system in 2024.](img/2024-yichen-studies.jpg)
:::
::::::::::::::

## Other Issues to Consider When Doing Evaluations

1. Explaining participants' rights and how their data will be handled.
    - Most institutions require researchers to disclose activities involving human participants (E.g., [ANU Human Research Ethics Committee](https://services.anu.edu.au/planning-governance/governance/anu-human-research-ethics-committee)).
2. Considering biases that may affect how you present findings.

> Dilemma

When is a person considered vulnerable, and how might this affect them?

- Charles can give an activity here.
- The book also has an in-depth activity here p.546.

# Planning Evaluations

## Issues during evaluation

- Ethical dimensions and consent
- Evaluation design and conduct:
    - __Reliability:__  “how well it produces the same results on separate occasions under the same circumstances”
    - __Validity:__ “whether the evaluation method measures what it intended to measure”
    - __Ecological validity:__  “how the environment in which an evaluation is conducted influences or distorts results”
    - __Bias:__ “occurs when the results are distorted”
    - __Scope:__ “how much of the findings can be generalised”


## Developing an evaluation plan

:::::::::::::: {.columns}
::: {.column width="60%"}
- Evaluation Goal/Aims
- Participants
- Setting
- Data to collect
- Methods
- Ethical Considerations/Consent Process
- Data capture/recording/storage
- Analysis method
- Output(s) of evaluation process
:::
::: {.column width="40%"}
![](img/09_evaluation_foundations_20.jpg)
:::
::::::::::::::

## Labs and Equipment

:::::::::::::: {.columns}
::: {.column width="60%"}

- Audio / Video recording devices,
- Speakers, tables,
- Experimental setup, etc.

- In-person / Remote (comparing to Minsik's focus group).
    - Zoom (e.g., COVID), etc.

- Charles can yap more.

:::
::: {.column width="40%"}
![Yichen Wang's human-AI musical collaboration research study setup at School of Music.](img/08_yichen_study_setup.jpg){width="100%"}
:::
::::::::::::::

## Conducting Experiments

<!-- Specific hypothesis tests predict how people will perform with an interface. -->

- Hypotheses Testing:
    - Examine the relationship between variables (independent vs. dependent);
    - Null and alternative hypotheses guide testing;
    - Careful experimental design is essential to control other variables and interpret results accurately!

- Experimental Design:
    - Choosing participants for conditions is critical 
    <!-- Notes for Charles -->
    <!-- prior exposure or learning can bias results, so care must be taken to avoid unfair advantages or training effects. -->
    - Three main types:
        1. Different-participant design
         <!-- uses separate groups for each condition, avoiding order effects but requiring more participants. -->
        2. Same-participant design
         <!-- has all participants do all conditions, reducing individual differences but requiring counterbalancing to avoid order effects. -->
        3. Matched-participant design 
        <!-- pairs participants based on shared traits to reduce variability, though unaccounted factors may still influence outcomes. -->
    - Design choice affects validity and reliability
        <!-- so researchers must balance control over variables, participant numbers, and potential biases when planning experiments -->

 Data collection: task performance, response / task time, errors, etc.

## Experimental Design

Advantages and Disadvantages of Different Allocations of Participants to Conditions

| Design | Advantages | Disadvantages |
|---|---|---|
| Different participants (between-participants design) | - No order effects | - Requires many participants<br>- Individual differences can affect results<br>- Random assignment helps minimize differences |
| Same participants (within-participants design) | - Eliminates individual differences between conditions | - Requires counterbalancing<br>- Risk of order effects (e.g., learning or fatigue) |
| Matched participants (pair-wise design) | - No order effects<br>- Reduces impact of individual differences | - Time-consuming to find matched pairs<br>- May miss other influential variables |


## Statistics: t-tests

- Widely used in HCI to compare means between two conditions (e.g., menu selection times) and test if differences are statistically significant.
- Include the t-value, degrees of freedom (df), and p-value, where df depends on participant numbers, and p indicates the probability the result is due to chance.
- `p < 0.05` typically means the difference is significant, allowing rejection of the null hypothesis, with smaller p-values (e.g., <0.01) indicating stronger evidence.

## In-the-Wild Studies

:::::::::::::: {.columns}
::: {.column width="60%"}
Evaluate technology in natural settings with minimal control over participants, reflecting real-world use but introducing unpredictability and complexity compared to lab studies.

Observations, interviews, logged usage, and experience sampling methods, capturing rich, contextual information about how people interact with technology in everyday life.

Ethical and practical challenges are greater, e.g., participant consent, privacy, equipment issues, and environmental factors.

> Reveal insights about actual use and long-term integration that lab studies often miss.

- Charles example the performance in the wild work by Steve Benford.

:::
::: {.column width="40%"}
![Overview of performance-led research in the wild. [@benford2013performancewild]](img/benford-performance-led-in-the-wild.jpg){width="100%"}
:::
::::::::::::::

 

# Evaluation by Inspection

![Image Source: Online ... I have this back in 2018 and just can't remember. -- Yichen](img/08_sticky_notes.jpg)


Skip the "users"! Just evaluate against established principles (heuristics) and standards.

## Expert Evaluation

- Conducted by designers and design “experts” rather than with end users
- Inspection methods – expert role plays user
- __Heuristic evaluation:__ Researchers evaluate whether aspects design adhere to established usability principles (see over)
- __Cognitive walkthroughs:__ Simulating user reasoning and problem solving at each step in an interaction sequence (evidence, availability, accessibility of correct action)
- __Analytics:__ Understanding user demographics and tracing activities (e.g., number of clicks, duration of sessions etc.)
- __A/B Testing:__ Large number of users assigned Design A or B and compare use to test “variable of interest” (e.g., number of clicks on advertising during test period)

## Heuristic Evaluations of User Interfaces (video)

![Using established principles (heuristics) to evaluate ([video](https://www.youtube.com/watch?v=6Bw0n6Jvwxk))](img/09_evaluation_foundations_10.png)

## Nielsen's 10 Usability Heuristics

:::::::::::::: {.columns}
::: {.column width="50%"}
1. **Visibility of system status:** keep the user informed
2. **Match between system and real world:** system uses language and communication familiar to the user, information is natural and logical
3. **User control and freedom:** users make mistakes, there should be "emergency exits" to cancel and return quickly
4. **Consistency and standards:** users should not wonder whether words, situations or actions mean the same thing, follow conventions
5. **Error prevention:** eliminate error-prone conditions, or check with user before they occur
:::
::: {.column width="50%"}
6. **Recognition rather than recall:** make elements, actions, and options visible
7. **Flexibility and efficienty of use:** shortcuts to speed up for experts, allow tailored experiences
8. **Aesthetic and minimal design:** less is more, no unnecessary information
9. **Help users recognise, diagnose and recover from errors:** error messages need plain language, and suggest solutions
10. **Help and documentation:** best if explanation is not needed, if it is, make it good
:::
::::::::::::::

<!-- evaluation of VR interface: https://www.nngroup.com/articles/usability-heuristics-virtual-reality/ -->
<!-- ![[Jakob's 10 Usability Heuristics](https://media.nngroup.com/media/articles/attachments/Heuristic_Summary1-compressed.pdf)](img/09_evaluation_foundations_11.png) -->
<!-- ![[Usability Heuristics 5-10](https://media.nngroup.com/media/articles/attachments/Heuristic_Summary1-compressed.pdf)](img/09_evaluation_foundations_12.png) -->

## Web Design Heuristics

:::::::::::::: {.columns}
::: {.column width="60%"}
@budd-web-design:2007 introduces further heuristics focussed on web, here's some from the [list](https://andybudd.com/archives/2007/01/heuristics_for_modern_web_application_de):

- **Clarity:** Make the system as clear, concise and meaningful as possible for the intended audience.
- **Minimise unneccessary complexity and cognitive load:** Make the system as simple as possible for people to accomplish their tasks.
- **Provide context:** Interfaces should provide people with a sense of context in time and space
- **Promote a pleasurable and positive experience:** people should be treated with respect and the design should be aesthetically pleasing and promote a pleasurable and rewarding experience


:::
::: {.column width="40%"}
![Evaluating a website. Image: [nngroup (link)](https://www.nngroup.com/articles/how-to-conduct-a-heuristic-evaluation/)](img/evaluation-nngroup-heuristic.jpg)
:::
::::::::::::::

## Shneiderman’s Eight Golden Rules of Design

1. Strive for consistency
2. Seek universal usability
3. Offer informative feedback
4. Design dialogs to yield closure
5. Prevent errors
6. Permit easy reversal of actions
7. Keep users in control
8. Reduce short-term memory load

## Analytics: What can you learn?

:::::::::::::: {.columns}
::: {.column width="50%"}
![](img/09_evaluation_foundations_16.png)

:::
::: {.column width="50%"}
![](img/09_evaluation_foundations_17.png)
:::
::::::::::::::

## A/B Testing    

:::::::::::::: {.columns}
::: {.column width="60%"}
- Large-scale, online controlled experiment used to compare two designs (A = control, B = new design) by measuring user behavior (e.g., click rates), often without users knowing they are part of a study.
- Between-participants design, randomly assigning users to different versions and analyzing outcomes statistically to determine if observed differences are due to the design and not chance.
- Proper setup is critical — running an A/A test first ensures the testing infrastructure is sound, and careful design is needed to avoid misleading results, as shown in real-world examples like Microsoft Office 2007.
:::
::: {.column width="40%"}
![Original ad title for buying flowers (top) and suggested new title design (below).
Source: Kohavi et al. (2022), Cambridge University Press](img/08_abtest.jpg){width="60%"}
:::
::::::::::::::

<!-- Charles: this  Kohavi et al. (2022) reference is cursed; i'll leave you to fix it. -->

## Predictive Models

Estimate user performance without needing real users, using formulas to assess task efficiency — useful in early design stages or when testing with users is difficult.

Fitts’ Law [@fitts1954information]:

- predicts how long it takes to point at a target based on its size and distance, helping designers optimize button placement, size, and spacing on screens and devices.

- wide applications in HCI, including evaluating input methods (e.g., touch, gaze, tilt), designing for mobile and VR, and simulating interactions for users with motor impairments.


## Evaluation after deployment: adoption, use, and non-use

:::::::::::::: {.columns}
::: {.column width="60%"}
- Adoption/Appropriation/Design-in-use [@ehn-participation:2008]
- Technology acceptance [@davis1989perceived]
- Non-use [@satchell-beyond-user:2009]
- Technology habitation [@soro-older-users:2016]
- Technology individuation [@ambe-individuation:2017]
:::
::: {.column width="40%"}
![(Ambe et al. 2017)](img/09_evaluation_foundations_19.jpg)
:::
::::::::::::::

# Questions: Who has a question?

:::::::::::::: {.columns}
::: {.column width="60%"}
**Who has a question?**

- I can take _cathchbox_ question up until 2:55
- For after class questions: meet me outside the classroom at the bar (for 30 minutes)
- Feel free to ask about **any aspect of the course**
- Also feel free to ask about **any aspect of computing at ANU**! I may not be able to help, but I can listen.

:::
::: {.column width="40%"}
![Meet you _at the bar_ for questions. 🍸🥤🫖☕️ Unfortunately no drinks served! 🙃](img/kambri-bar.jpg)
:::
::::::::::::::

# References {.allowframebreaks}
